{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Net\n",
    "\n",
    "Now that we've got all the data, in arrays, and normalized, it's time to train a neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from IPython.display import display, Markdown\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15000, 128, 128), (15000, 128, 128)\n",
      "Test shape: (2500, 128, 128), (2500, 128, 128)\n",
      "Valid shape: (2500, 128, 128), (2500, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "with open('data/combined.pickle', 'rb') as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "X_train = data_dict['X_train']\n",
    "y_train = data_dict['y_train']\n",
    "X_test = data_dict['X_test']\n",
    "y_test = data_dict['y_test']\n",
    "X_valid = data_dict['X_valid']\n",
    "y_valid = data_dict['y_valid']\n",
    "\n",
    "del data_dict # Free all the mallocs!\n",
    "\n",
    "print(\"Train shape: %s, %s\" % (X_train.shape, y_train.shape))\n",
    "print(\"Test shape: %s, %s\" % (X_test.shape, y_test.shape))\n",
    "print(\"Valid shape: %s, %s\" % (X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat tensor\n",
    "So it will fit our encoder/decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (15000, 128, 128), (15000, 128, 128)\n",
      "Test shape: (2500, 16384), (2500, 16384)\n",
      "Valid shape: (2500, 16384), (2500, 16384)\n"
     ]
    }
   ],
   "source": [
    "num_channels = 1 # only grey\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, dataset.shape[1] * dataset.shape[2])).astype(np.float32)\n",
    "    labels = labels.reshape((-1, labels.shape[1] * labels.shape[2])).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "print(\"Train shape: %s, %s\" % (X_train.shape, y_train.shape))\n",
    "X_train, y_train = reformat(X_train, y_train)\n",
    "X_test, y_test = reformat(X_test, y_test)\n",
    "X_valid, y_valid = reformat(X_valid, y_valid)\n",
    "\n",
    "print(\"Test shape: %s, %s\" % (X_test.shape, y_test.shape))\n",
    "print(\"Valid shape: %s, %s\" % (X_valid.shape, y_valid.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden parameters\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 128 # 2nd layer num features\n",
    "n_input = X_train.shape[1] # data shape\n",
    "\n",
    "# parameters\n",
    "intial_stddev = 0.1\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# activation\n",
    "activate = tf.nn.sigmoid\n",
    "# activate = tf.nn.relu\n",
    "# activate = tf.tanh\n",
    "\n",
    "# This neural net is composed of 2 steps,\n",
    "# the first it encodes the data, with 1 hidden layer,\n",
    "# the second does the opposite, converts it back to\n",
    "# the original shape\n",
    "\n",
    "# Encode\n",
    "def encode(x):\n",
    "    # First layer\n",
    "    layer_1 = activate(tf.add(tf.matmul(x, W['encoder_h1']), b['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = activate(tf.add(tf.matmul(layer_1, W['encoder_h2']), b['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Decode\n",
    "def decode(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = activate(tf.add(tf.matmul(x, W['decoder_h1']), b['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = activate(tf.add(tf.matmul(layer_1, W['decoder_h2']), b['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # input variables\n",
    "    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, n_input))\n",
    "    tf_X_valid = tf.constant(X_valid)\n",
    "    tf_X_test = tf.constant(X_test)\n",
    "\n",
    "    # weights / slopes\n",
    "    W = {\n",
    "        'encoder_h1': tf.Variable(tf.truncated_normal([n_input, n_hidden_1], stddev=intial_stddev)),\n",
    "        'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev=intial_stddev)),\n",
    "        'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_1], stddev=intial_stddev)),\n",
    "        'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_input], stddev=intial_stddev))\n",
    "    }\n",
    "    # biases / intercepts\n",
    "    b = {\n",
    "        'encoder_b1': tf.Variable(tf.truncated_normal([n_hidden_1], stddev=intial_stddev)),\n",
    "        'encoder_b2': tf.Variable(tf.truncated_normal([n_hidden_2], stddev=intial_stddev)),\n",
    "        'decoder_b1': tf.Variable(tf.truncated_normal([n_hidden_1], stddev=intial_stddev)),\n",
    "        'decoder_b2': tf.Variable(tf.truncated_normal([n_input], stddev=intial_stddev))\n",
    "    }\n",
    "    \n",
    "    # Execute sequence of encoding -> decoding to\n",
    "    # generate predictions\n",
    "    y_pred = decode(encode(tf_X_train))\n",
    "    \n",
    "    # Our goal is to reduce the mean between the originals and the predictions.\n",
    "    # This value is squared to be more aggressive with distant values\n",
    "    loss = tf.reduce_mean(tf.pow(tf_y_train - y_pred, 2))\n",
    "    \n",
    "    # Learning rate with exponential decay\n",
    "    global_step = tf.Variable(0, trainable=False) \n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy is the difference between both images\n",
    "def accuracy(predictions, labels):\n",
    "    return 1. - np.mean(np.abs(predictions - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      ".................................................."
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'valid_prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-212de31f305e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#             m_acc = accuracy(predictions, batch_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#             minibatch_acc.append(m_acc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_prediction' is not defined"
     ]
    }
   ],
   "source": [
    "keep_training = True\n",
    "\n",
    "# Accuracy arrays to draw a chart\n",
    "minibatch_acc = [];\n",
    "val_acc = [];\n",
    "test_acc = [];\n",
    "\n",
    "def show_stats():\n",
    "    clear_output()\n",
    "    handle1, = plt.plot(minibatch_acc, label=\"train score\")\n",
    "    handle2, = plt.plot(val_acc, label=\"validation score\")\n",
    "    plt.legend([handle1, handle2], loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    step = 0;\n",
    "    while step < 1000:\n",
    "        print(\".\", end=\"\")\n",
    "        \n",
    "        # SGD\n",
    "        offset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "        batch_data = X_train[offset:(offset + batch_size), :]\n",
    "        batch_labels = y_train[offset:(offset + batch_size), :]\n",
    "        \n",
    "        # Data passed into the session\n",
    "        feed_dict = {\n",
    "            tf_X_train : batch_data, \n",
    "            tf_y_train : batch_labels,\n",
    "            global_step: step\n",
    "        }\n",
    "        \n",
    "        # Run session\n",
    "        _, l = session.run(\n",
    "          [optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        # Increment global step to decrease learning ratio\n",
    "        step = step + 1\n",
    "        \n",
    "        if (step % 50 == 0):\n",
    "            m_acc = accuracy(predictions, batch_labels)\n",
    "            minibatch_acc.append(m_acc)\n",
    "            v_acc = accuracy(valid_prediction.eval(), y_valid)\n",
    "            val_acc.append(v_acc)\n",
    "\n",
    "            clear_output()\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f\" % m_acc)\n",
    "            print(\"Validation accuracy: %.1f\" % v_acc)\n",
    "\n",
    "    show_stats()\n",
    "    print('Test accuracy: %.1f' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this snippet to check the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
